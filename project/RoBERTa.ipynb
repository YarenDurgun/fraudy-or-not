{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# For the RAM issues\n",
        "\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w23nW844jrkV",
        "outputId": "7ee944cf-e8fd-473c-ef8f-92044dc6f6d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FG45Ha9lXOo-",
        "outputId": "7239c2d0-cc04-4779-e326-f687537fc75f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from datasets import Dataset\n",
        "from tqdm import tqdm\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, TrainingArguments, Trainer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ykp1qu8XoKh",
        "outputId": "f6285e4b-4b1d-4ed1-a3cd-b9aa060ba32f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "iY0YH3V8Ygf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir = '/content/gdrive/MyDrive/Essentials in Text and Speech Processing'\n",
        "csv_path = os.path.join(dataset_dir, 'fake_job_postings.csv')\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Drop features we know are useless from statistical model preprocessing\n",
        "df = df.drop(['job_id'], axis=1)\n",
        "df = df.drop(['salary_range'], axis=1)\n",
        "df = df.drop(['benefits'], axis=1)\n",
        "df = df.drop(['department'], axis=1)"
      ],
      "metadata": {
        "id": "axtwFX_4YfPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The 'location' feature has inconsistencies so we standardize the format by splitting the 'location' column into Country, State, and City\n",
        "def split_location(location):\n",
        "    # Split the location by commas\n",
        "    parts = location.split(',')\n",
        "\n",
        "    # Ensure the list has 3 elements (Country, State, City), fill missing with 'Unknown'\n",
        "    while len(parts) < 3:\n",
        "        parts.append('unknown')  # Add 'Unknown' for missing elements\n",
        "\n",
        "    return parts[:3]  # In case there are more than 3 parts\n",
        "\n",
        "# Make sure there are no missing values in 'location'\n",
        "df['location'] = df['location'].fillna('unknown')\n",
        "\n",
        "# Apply the function to the 'location' column\n",
        "df[['country', 'state', 'city']] = df['location'].apply(lambda loc: split_location(loc)).apply(pd.Series)\n",
        "\n",
        "# Now fill any remaining NaN values with 'unknown' just in case\n",
        "df[['country', 'state', 'city']] = df[['country', 'state', 'city']].fillna('Unknown')\n",
        "\n",
        "# Drop the location feature as it is useles now\n",
        "df = df.drop(['location'], axis=1)\n",
        "\n",
        "# Insert \"unknown\" in place of null values\n",
        "df = df.fillna('unknown')\n",
        "\n",
        "# Lowercasing\n",
        "df = df.apply(lambda col: col.map(lambda x: x.lower() if isinstance(x, str) else x))\n",
        "\n",
        "# Remove punctuation marks and special characters\n",
        "df = df.apply(lambda col: col.map(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x) if isinstance(x, str) else x))\n",
        "\n",
        "x = df.drop(['fraudulent'], axis=1) # Features\n",
        "y = df['fraudulent'] # Labels\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Tp4Vr7uJYmqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained RoBERTa tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "# Preprocess the text features (combine columns)\n",
        "text_columns = ['title', 'company_profile', 'description', 'requirements', 'industry', 'function', 'required_experience', 'required_education', 'employment_type', 'city', 'state', 'country']\n",
        "\n",
        "# Apply a function to combine these features into one column\n",
        "x_train_text = x_train[text_columns].apply(lambda row: ' '.join(row), axis=1)\n",
        "x_test_text = x_test[text_columns].apply(lambda row: ' '.join(row), axis=1)\n",
        "\n",
        "# Function to tokenize and pad sequences to a fixed length\n",
        "def tokenize_and_pad(texts, tokenizer, max_length=512):\n",
        "    return tokenizer(texts.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
        "\n",
        "# Tokenize the text data for both train and test sets\n",
        "train_encodings = tokenize_and_pad(x_train_text, tokenizer)\n",
        "test_encodings = tokenize_and_pad(x_test_text, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSINvr_Hakvj",
        "outputId": "c0918211-2e4a-4f20-cd07-6914ffeeda1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Compute class weights based on the training labels\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "# Update model's loss function to include class weights\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Ensure that y_train and y_test are tensors and not pandas Series\n",
        "y_train_tensor = torch.tensor(y_train.values).to(device)\n",
        "y_test_tensor = torch.tensor(y_test.values).to(device)\n",
        "\n",
        "# Custom dataset class with correct label handling\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = self.labels[idx]  # Make sure labels are correctly passed as tensors\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Create the Dataset for train and test sets\n",
        "train_dataset = CustomDataset(train_encodings, y_train_tensor)\n",
        "test_dataset = CustomDataset(test_encodings, y_test_tensor)\n",
        "\n",
        "# Create DataLoader for train and test sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)"
      ],
      "metadata": {
        "id": "T6iQx7Tsa5j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained RoBERTa model for sequence classification\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)  # binary classification (fraudulent vs non-fraudulent)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpM5ftdma9tZ",
        "outputId": "f2c22df4-8ef7-4fab-85b4-0af63a91da97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Modify the training loop to use the custom criterion\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_loader):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}  # Move to GPU\n",
        "        optimizer.zero_grad()  # Reset gradients\n",
        "\n",
        "        # Forward pass with the custom criterion\n",
        "        outputs = model(**batch)  # Forward pass\n",
        "        logits = outputs.logits\n",
        "        loss = criterion(logits, batch['labels'])  # Compute the weighted loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update model weights\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUtXXoC0bsgb",
        "outputId": "2e029910-3b61-4221-84c3-63af60f11a77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/894 [00:00<?, ?it/s]<ipython-input-8-6d3bbd38f85e>:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation loop\n",
        "model.eval()\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "for batch in tqdm(test_loader):\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    preds = torch.argmax(logits, dim=1).cpu().numpy()  # Convert logits to predicted class labels\n",
        "    predictions.extend(preds)\n",
        "    true_labels.extend(batch['labels'].cpu().numpy())"
      ],
      "metadata": {
        "id": "_JZ2js0jb6MP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get accuracy and classification report\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "report = classification_report(true_labels, predictions)\n",
        "conf_matrix = confusion_matrix(true_labels, predictions)"
      ],
      "metadata": {
        "id": "Q6NFy0yOchMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = '/content/gdrive/MyDrive/Essentials in Text and Speech Processing/Results'\n",
        "\n",
        "# Save the SVM model just in case\n",
        "with open(os.path.join(output_dir, 'roberta_model.pkl'), 'wb') as model_file:\n",
        "    pickle.dump(model, model_file)\n",
        "\n",
        "# Save the evaluation results just in case\n",
        "with open(os.path.join(output_dir, 'roberta_classification_report.txt'), 'w') as report_file:\n",
        "    report_file.write(\"Classification Report:\\n\")\n",
        "    report_file.write(report + \"\\n\")\n",
        "\n",
        "    report_file.write(\"Accuracy:\\n\")\n",
        "    report_file.write(f\"{accuracy:.4f}\\n\\n\")\n",
        "\n",
        "    report_file.write(\"Confusion Matrix:\\n\")\n",
        "    for row in conf_matrix:\n",
        "        report_file.write(\" \".join(map(str, row)) + \"\\n\")\n",
        "\n",
        "print(\"Model and classification report have been saved.\")"
      ],
      "metadata": {
        "id": "AiqPnhvacMj-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}